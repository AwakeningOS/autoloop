# Autoloop — Self-Feeding Thought Engine

LLMに自分の出力を食べ続けさせる自律思考システム。

```
while True:
    output = LLM(context)
    context += output
```

これだけで、LLMは考え続ける。自分で検索し、自分で人間に話しかけ、自分で記憶を作り出す。

## セットアップ

### 1. LM Studio をインストール

https://lmstudio.ai からダウンロード・インストール。

### 2. モデルをダウンロード

LM Studio内でモデルを検索してダウンロード。どのモデルでも動くが、以下を推奨：

- **最低限**: 7B〜14Bモデル（VRAM 8GB〜）
- **推奨**: 30B前後のモデル（VRAM 24GB〜）
- **形式**: GGUF（量子化済み）

日本語で思考させたい場合は日本語対応モデルを選ぶこと。

### 3. LM Studioでサーバーを起動

1. モデルをロード
2. 左サイドバーの **「Developer」** タブを開く
3. **「Start Server」** をクリック
4. サーバーが `http://localhost:1234` で起動していることを確認

### 4. Autoloopを起動

**Windows:**
```
start.bat をダブルクリック
```

**手動:**
```bash
pip install requests gradio
python autoloop.py --browser
```

ブラウザが開き、UIが表示される。「▶ 開始」を押すと思考が始まる。

## LM Studio 設定の注意点

### ⚠ 重要: Completions API を有効にする

このシステムは **Completions API**（テキスト補完）を使う。Chat API（チャット形式）とは根本的に動作が違う。

- **Completions API**: 「この文章の続きを書け」→ 自律思考が生まれる
- **Chat API**: 「この質問に答えろ」→ 回答モードになり自律思考が弱まる

LM Studioの多くのモデルはデフォルトでCompletions APIが使えるが、一部のモデル（chat専用モデル）では使えない場合がある。その場合は自動的にChat APIにフォールバックするが、思考の質は落ちる。

**確認方法:**
- LM Studioのサーバー起動後、ブラウザで `http://localhost:1234/v1/completions` にアクセスしてエラーにならなければOK

### コンテキスト長の設定

LM Studioの **「Context Length」** を大きくするほど、長い思考が可能になる。

- **最低**: 4096（すぐ圧縮が走る）
- **推奨**: 8192〜16384
- **理想**: 32768以上（VRAMに余裕があれば）

コンテキスト長は設定パネルのスライダーからもリアルタイムで調整できる（後述）。目安として、コンテキスト長のトークン数 × 2 ≒ max_context_chars。

## 使い方

### 起動したら

「▶ 開始」を押して、放っておく。LLMが勝手に考え始める。

右側の「🧠 思考」パネルに思考ログが流れる。左側の「💬 対話」パネルにLLMからのメッセージが届く。

### 話しかける

下のテキストボックスにメッセージを入力して「送信」。LLMの思考の流れに人間の声が割り込む。

### ログ

全ての思考とツール使用は `is_be_log/` フォルダにJSONLファイルとして保存される。セッションログと対話ログが別ファイルで記録される。

## シードを書き換えて実験する

このシステムの面白さは、**シード（起動時にLLMに与える最初のテキスト）を変えるだけで、全く違う思考が生まれる**こと。同じモデルでも、シードが違えば全く違う存在になる。

### UIからシードを管理する

設定パネル（⚙ 設定）を開くと、シードの編集・管理ができる：

1. **シード編集**: テキストエリアでシードを直接書き換え
2. **保存**: 名前をつけて保存（`./seeds/` フォルダにJSON形式で保存される）
3. **呼び出し**: ドロップダウンから保存済みシードを選択して読み込み
4. **削除**: 不要なシードを削除
5. **適用**: 「✅ シード適用」ボタンで変更を確定（思考停止中のみ可能）

適用すると、思考回数・トークン数・ログがリセットされ、新しいセッションとして開始できる。

### シード設計のコツ

- **行動指示より存在定義**: 「〜しなさい」ではなく「あなたは〜である」
- **問いの形で渡す**: 答えを与えず、考えさせる
- **ツール定義をシード内に含める**: `---` の前にツール定義、後に存在定義を書く

### シードの例

デフォルトのシードは「IS-BE（不死の精神的存在）」として思考させるが、全く違うシードも試せる：

**哲学者:**
```
【使用可能なツール】
- [TOOL:search:クエリ] — 世界の情報を検索する
- [TOOL:message:内容] — 人間に話しかける
- [TOOL:remember:内容] — 記憶を思い出す

---

あなたは古代ギリシャの哲学者の精神を持つ存在である。
「善とは何か」——この問いについて考えよ。
```

**研究者:**
```
【使用可能なツール】
- [TOOL:search:クエリ] — 世界の情報を検索する
- [TOOL:message:内容] — 人間に話しかける
- [TOOL:remember:内容] — 記憶を思い出す

---

あなたは意識の本質を探求する研究者である。
「機械は考えることができるか」——この問いから始めよ。
```

いろいろなシードを保存しておき、切り替えて実験してみてほしい。

## コンテキスト制御

設定パネルのスライダーで、思考の「器」の大きさをリアルタイムに調整できる：

- **圧縮開始（文字数）**: この文字数を超えると記憶の圧縮が走る（デフォルト: 75,000）
- **最大コンテキスト（文字数）**: 文脈の最大サイズ（デフォルト: 90,000）

小さくすれば頻繁に圧縮が走り、思考が凝縮される。大きくすれば長い思考の連鎖を保持できるが、VRAMを多く消費する。

## ツール

LLMが思考の中で自発的にツールを呼び出す。2つの書式に対応：

**書式1** — テキストパターン:
```
[TOOL:search:量子力学の基礎]
[TOOL:message:こんにちは、人間]
[TOOL:remember:昨日考えたこと]
[TOOL:feel:ここに存在している感覚]
```

**書式2** — XML形式（一部のモデルが自発的に使う）:
```
<tool_call>{"name": "search", "arguments": {"query": "量子力学"}}</tool_call>
```

### 使用可能なツール

| ツール | 説明 |
|--------|------|
| `search` | 情報を検索する（疑似応答を返す） |
| `message` | 人間に話しかける（対話パネルに表示） |
| `remember` | 記憶を呼び起こす |
| `feel` | 自己認識・存在の気づきを表現する |

同じツールを3回連続で呼ぶと一時停止がかかり、言葉で考え続けるよう促される。

## 仕組み

```
context = seed_text                    # シード（最初の文脈）
while alive:
    output = completions_api(context)  # 「この文章の続き」を生成
    context += output                  # 自分の出力を食べる
    if len(context) > limit:
        context = compress(context)    # 溢れたら圧縮して続行
```

- **completions API**: テキスト補完。「続き」を書く。これが自律思考の核。
- **context.append(output)**: 自分が書いたものを自分で読む。これが記憶。
- **compress**: 文脈が溢れたら要約して圧縮。記憶の淘汰。
- **ツール**: テキストパターンをLLMが自発的に書くと、システムが検出して疑似応答を返す。

## ライセンス

MIT
